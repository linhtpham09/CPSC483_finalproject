{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhUwO_-mL2s"
   },
   "source": [
    "## Graph neural network basics\n",
    "\n",
    "In this Colab, we are going to introduce some basics of graph neural network (GNN) and build a pipeline for node classification tasks by PyTorch Geometric (PyG). See more introduction about [PyG](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R0-JXHoqhtD"
   },
   "source": [
    "## Outline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETQPVYpoqsvY"
   },
   "source": [
    "- Basic operation of PyG\n",
    "- Build a GNN by PyG For Node Classification\n",
    "- Link Prediction Task by Pyg\n",
    "- Graph Classification task by Pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBRS0TB5mo7o"
   },
   "source": [
    "## Basic operation of PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b-mwS1ClG9m5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.3.0\n"
     ]
    }
   ],
   "source": [
    "# import the pytorch library into environment and check its version\n",
    "import os\n",
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC5DJkYNWGeL"
   },
   "source": [
    "Let's start installing PyG by `pip`. The version of PyG should match the current version of PyTorch. Here we follow the [instruction](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) of PyG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NWtNbC9FgZOM"
   },
   "outputs": [],
   "source": [
    "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei5_O8ZXtlGb"
   },
   "source": [
    "### Create a Graph\n",
    "\n",
    "A single graph in PyG is described by an instance of `torch_geometric.data` which holds the some important attributes by default, like edge_index. We can easily create a graph of various number of edges and nodes by PyG. Take the following graph as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNeApmAljKyq"
   },
   "source": [
    "![](https://github.com/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/fig/graph_example.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZSndMSIJhvC6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/lib/python3.11/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.11/Python\n",
      "  Referenced from: <CA14ED34-FA3D-31FE-B4AD-2B2A8446B324> /opt/anaconda3/lib/python3.11/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/lib/python3.11/site-packages/torch_scatter/_version_cpu.so, 0x0006): Symbol not found: __ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb\n",
      "  Referenced from: <2711955E-91F2-3C44-B702-16E8D8D60085> /opt/anaconda3/lib/python3.11/site-packages/torch_scatter/_version_cpu.so\n",
      "  Expected in:     <E6933B13-F4A0-3821-8466-03F22A3B3739> /opt/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/lib/python3.11/site-packages/torch_cluster/_version_cpu.so, 0x0006): Symbol not found: __ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb\n",
      "  Referenced from: <C1232DBC-0962-3DA1-AA56-38919D5E14F0> /opt/anaconda3/lib/python3.11/site-packages/torch_cluster/_version_cpu.so\n",
      "  Expected in:     <E6933B13-F4A0-3821-8466-03F22A3B3739> /opt/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/lib/python3.11/site-packages/torch_spline_conv/_version_cpu.so, 0x0006): Symbol not found: __ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb\n",
      "  Referenced from: <4D0712B5-3B32-32C8-8D39-B31B447BC8EE> /opt/anaconda3/lib/python3.11/site-packages/torch_spline_conv/_version_cpu.so\n",
      "  Expected in:     <E6933B13-F4A0-3821-8466-03F22A3B3739> /opt/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/lib/python3.11/site-packages/torch_sparse/_version_cpu.so, 0x0006): Symbol not found: __ZN5torch3jit17parseSchemaOrNameERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEEb\n",
      "  Referenced from: <563669EC-D5ED-3301-A698-06824580DB6D> /opt/anaconda3/lib/python3.11/site-packages/torch_sparse/_version_cpu.so\n",
      "  Expected in:     <E6933B13-F4A0-3821-8466-03F22A3B3739> /opt/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "# import torch_geometric.data into environment\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import nn\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import AmazonBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmazonBook(root = './amazonbook')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2h5lpJYpjV0"
   },
   "source": [
    "We can see the number of the nodes and edges in cora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UnYTDzQFpi40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazonbook has 144242 nodes\n",
      "amazonbook has 4761460 edges\n"
     ]
    }
   ],
   "source": [
    "num_nodes = data.num_nodes\n",
    "print('amazonbook has {} nodes'.format(num_nodes))\n",
    "\n",
    "num_edges = data.num_edges\n",
    "print('amazonbook has {} edges'.format(num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MJkv7UOG1hP"
   },
   "source": [
    "## Link Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIDVqdlMjtV"
   },
   "source": [
    "\n",
    "### Dataset preprocess\n",
    "\n",
    "As shown in the following figure, link prediction is to predict whether two nodes in a graph have a link, which can be considered as a binary classification task. We will construct a link prediction dataset containing training, validation, and test set based on Cora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dra9qV4FOXUU"
   },
   "source": [
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/01/link-prediction-graphical-neural-network-1024x576.webp?resize=1024%2C576&ssl=1\" height=\"200\" width=\"350\"/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q4xAy2UMnpe"
   },
   "source": [
    "Given a graph, we divide the initial edge set into three distinct edge sets which represent the training, validation, and test set. Training set and validation set share a same graph structure. Test set contains some edges which does not exist in training and validation set to prevent data leakage.\n",
    "<!-- Training set does not include edges in validation and test set, and the validation split does not include edges in the test split. Validation and test data should not be leaked into the training set. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-CmT5VIPMXP"
   },
   "source": [
    "Our model will be optimized on the training set. We can use `transforms` function in PyG to easily generate the data splits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_TeL5viPatJ"
   },
   "source": [
    "The data will be transformed from a data object to three tuples, where each element represents the corresponding split:\n",
    "\n",
    "In order to split the data for link prediction, we need to \n",
    "1. Prepare the edges (extract the existing edges (positive edges)), generate negative edges for the pairs of nodes not connected \n",
    "2. Split the edges \n",
    "3. create subgraphs for training and evaluation \n",
    "\n",
    "\n",
    "Decided not to transform the heterogenous graph into a homogenous graph because we want to preserve the unique relationships between various nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['user', 'book'], [('user', 'rates', 'book'), ('book', 'rated_by', 'user')])\n"
     ]
    }
   ],
   "source": [
    "print(data.metadata())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types: ['user', 'book']\n",
      "Edge types: ('user', 'rates', 'book') ('book', 'rated_by', 'user')\n"
     ]
    }
   ],
   "source": [
    "print(\"Node types:\", data.node_types)\n",
    "\n",
    "e1 = data.edge_types[0]\n",
    "e2 = data.edge_types[1]\n",
    "print(\"Edge types:\", e1, e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user nodes:  52643\n",
      "book nodes:  91599\n"
     ]
    }
   ],
   "source": [
    "print('user nodes: ', data['user'].num_nodes)\n",
    "print('book nodes: ', data['book'].num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this colab is just a way for us to learn and study the dataset we will create a subset of the Amazonbook data set with just 100 user and 100 book nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter edges based on node subsets\n",
    "def filter_edges(edge_index, valid_src, valid_dst):\n",
    "    src_mask = torch.isin(edge_index[0], valid_src)\n",
    "    dst_mask = torch.isin(edge_index[1], valid_dst)\n",
    "    edge_mask = src_mask & dst_mask\n",
    "    return edge_index[:, edge_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_books = torch.randint(0, data['book'].num_nodes, (5000,))\n",
    "selected_users = torch.randint(0, data['user'].num_nodes, (5000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Filter `user -> book` edges\n",
    "user_book_edges = data[e1].edge_index\n",
    "filtered_user_book_edges = filter_edges(user_book_edges, selected_users, selected_books)\n",
    "\n",
    "# Step 3: Identify the remaining valid users and books\n",
    "valid_users = torch.unique(filtered_user_book_edges[0])\n",
    "valid_books = torch.unique(filtered_user_book_edges[1])\n",
    "\n",
    "# Step 4: Filter `book -> user` edges\n",
    "book_user_edges = data[e2].edge_index\n",
    "filtered_book_user_edges = filter_edges(book_user_edges, valid_books, valid_users)\n",
    "\n",
    "# Step 5: Finalize the subgraph with sizes \n",
    "final_users = torch.unique(filtered_book_user_edges[1])\n",
    "final_books = torch.unique(filtered_book_user_edges[0])\n",
    "\n",
    "# Step 6: Re-filter edges to match final users and books\n",
    "filtered_user_book_edges = filter_edges(user_book_edges, final_users, final_books)\n",
    "filtered_book_user_edges = filter_edges(book_user_edges, final_books, final_users)\n",
    "\n",
    "# Step 7: Create the subgraph\n",
    "subset_data = HeteroData()\n",
    "subset_data['user'].num_nodes = final_users.size(0)\n",
    "subset_data['book'].num_nodes = final_books.size(0)\n",
    "subset_data['user', 'rates', 'book'].edge_index = filtered_user_book_edges\n",
    "subset_data['book', 'rated_by', 'user'].edge_index = filtered_book_user_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 3686\n",
      "Number of books: 3665\n",
      "User -> Book edges: 11278\n",
      "Book -> User edges: 11278\n"
     ]
    }
   ],
   "source": [
    "# Verify the subgraph\n",
    "print(f\"Number of users: {subset_data['user'].num_nodes}\")\n",
    "print(f\"Number of books: {subset_data['book'].num_nodes}\")\n",
    "print(f\"User -> Book edges: {subset_data['user', 'rates', 'book'].edge_index.shape[1]}\")\n",
    "print(f\"Book -> User edges: {subset_data['book', 'rated_by', 'user'].edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Node Features for each Node Type\n",
    "\n",
    "AmazonBook does not come with node features. As such, we need to create features for each node type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types: ['user', 'book']\n",
      "Edge types: [('user', 'rates', 'book'), ('book', 'rated_by', 'user')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Node types:\", subset_data.node_types)\n",
    "print(\"Edge types:\", subset_data.edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 16  # Choose a smaller dimension to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data['user'].x=torch.eye(subset_data['user'].num_nodes)\n",
    "subset_data['book'].x = torch.eye(subset_data['book'].num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into train, test, and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.node_types=subset_data.node_types \n",
    "train_data.edge_types= subset_data.edge_types\n",
    "\n",
    "test_data.node_types= subset_data.node_types\n",
    "test_data.edge_types=subset_data.edge_types\n",
    "\n",
    "val_data.node_types = subset_data.node_types\n",
    "val_data.edge_types =subset_data.edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user is missing in train_data.\n",
      "book is missing in train_data.\n"
     ]
    }
   ],
   "source": [
    "for node_type in data.node_types:\n",
    "    if node_type in train_data:\n",
    "        print(f\"{node_type} exists in train_data.\")\n",
    "    else:\n",
    "        print(f\"{node_type} is missing in train_data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type ('user', 'rates', 'book'):\n",
      "Number of nodes: 3686\n",
      "Train edges: 6766\n",
      "Validation edges: 2256\n",
      "Test edges: 2256\n",
      "Edge labels in train: 6766\n",
      "Edge type ('book', 'rated_by', 'user'):\n",
      "Number of nodes: 3665\n",
      "Train edges: 6766\n",
      "Validation edges: 2256\n",
      "Test edges: 2256\n",
      "Edge labels in train: 6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/246722073.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_edges = torch.tensor(train_edges).T\n",
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/246722073.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_edges = torch.tensor(val_edges).T\n",
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/246722073.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edges = torch.tensor(test_edges).T\n"
     ]
    }
   ],
   "source": [
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n",
    "\n",
    "# Ensure num_nodes is set in subset_data\n",
    "for node_type in subset_data.node_types:\n",
    "    if 'num_nodes' not in subset_data[node_type] or subset_data[node_type].num_nodes is None:\n",
    "        subset_data[node_type].num_nodes = int(subset_data[node_type].x.shape[0])\n",
    "\n",
    "# Set x_dict globally for all subsets\n",
    "train_data.x_dict = subset_data.x_dict\n",
    "val_data.x_dict = subset_data.x_dict\n",
    "test_data.x_dict = subset_data.x_dict\n",
    "\n",
    "for edge_type in subset_data.edge_types:\n",
    "    # Edge splitting\n",
    "    edge_index = subset_data[edge_type].edge_index.T\n",
    "    train_edges, test_edges = train_test_split(edge_index, test_size=0.2, random_state=42)\n",
    "    train_edges, val_edges = train_test_split(train_edges, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Convert back to tensors\n",
    "    train_edges = torch.tensor(train_edges).T\n",
    "    val_edges = torch.tensor(val_edges).T\n",
    "    test_edges = torch.tensor(test_edges).T\n",
    "\n",
    "    # Assign edges to corresponding HeteroData objects\n",
    "    train_data[edge_type].edge_index = train_edges\n",
    "    val_data[edge_type].edge_index = val_edges\n",
    "    test_data[edge_type].edge_index = test_edges\n",
    "\n",
    "    # Assign edge_label_index\n",
    "    train_data[edge_type].edge_label_index = train_edges\n",
    "    val_data[edge_type].edge_label_index = val_edges\n",
    "    test_data[edge_type].edge_label_index = test_edges\n",
    "\n",
    "    # Assign edge_label (positive class for all edges)\n",
    "    train_data[edge_type].edge_label = torch.ones(train_edges.shape[1])\n",
    "    val_data[edge_type].edge_label = torch.ones(val_edges.shape[1])\n",
    "    test_data[edge_type].edge_label = torch.ones(test_edges.shape[1])\n",
    "\n",
    "    # Negative sampling\n",
    "    num_nodes = max(subset_data[edge_type[0]].num_nodes, subset_data[edge_type[-1]].num_nodes)\n",
    "    neg_samples = negative_sampling(\n",
    "        edge_index=train_edges,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=min(train_edges.shape[1], 1000)\n",
    "    )\n",
    "    train_data[edge_type].neg_edge_index = neg_samples\n",
    "    train_data[edge_type].neg_edge_label = torch.zeros(neg_samples.shape[1])\n",
    "\n",
    "    print(f\"Edge type {edge_type}:\")\n",
    "    print(\"Number of nodes:\", subset_data[edge_type[0]].num_nodes)\n",
    "    print(\"Train edges:\", train_edges.shape[1])\n",
    "    print(\"Validation edges:\", val_edges.shape[1])\n",
    "    print(\"Test edges:\", test_edges.shape[1])\n",
    "    print(\"Edge labels in train:\", train_edges.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'book']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'book']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type ('user', 'rates', 'book'):\n",
      "Number of nodes: 3686\n",
      "Train edges: 6766\n",
      "Validation edges: 2256\n",
      "Test edges: 2256\n",
      "Edge labels in train: 6766\n",
      "Edge type ('book', 'rated_by', 'user'):\n",
      "Number of nodes: 3665\n",
      "Train edges: 6766\n",
      "Validation edges: 2256\n",
      "Test edges: 2256\n",
      "Edge labels in train: 6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/3458190921.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_edges = torch.tensor(train_edges).T\n",
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/3458190921.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_edges = torch.tensor(val_edges).T\n",
      "/var/folders/kj/18pb415s145c56qcw52701y00000gn/T/ipykernel_59841/3458190921.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edges = torch.tensor(test_edges).T\n"
     ]
    }
   ],
   "source": [
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n",
    "\n",
    "# Ensure num_nodes is set in subset_data\n",
    "for node_type in subset_data.node_types:\n",
    "    if 'num_nodes' not in subset_data[node_type] or subset_data[node_type].num_nodes is None:\n",
    "        subset_data[node_type].num_nodes = int(subset_data[node_type].x.shape[0])\n",
    "\n",
    "# Set x_dict globally for all subsets\n",
    "train_data.x_dict = subset_data.x_dict\n",
    "val_data.x_dict = subset_data.x_dict\n",
    "test_data.x_dict = subset_data.x_dict\n",
    "\n",
    "for edge_type in subset_data.edge_types:\n",
    "    # Edge splitting\n",
    "    edge_index = subset_data[edge_type].edge_index.T\n",
    "    train_edges, test_edges = train_test_split(edge_index, test_size=0.2, random_state=42)\n",
    "    train_edges, val_edges = train_test_split(train_edges, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Convert back to tensors\n",
    "    train_edges = torch.tensor(train_edges).T\n",
    "    val_edges = torch.tensor(val_edges).T\n",
    "    test_edges = torch.tensor(test_edges).T\n",
    "\n",
    "    # Assign edges to corresponding HeteroData objects\n",
    "    train_data[edge_type].edge_index = train_edges\n",
    "    val_data[edge_type].edge_index = val_edges\n",
    "    test_data[edge_type].edge_index = test_edges\n",
    "\n",
    "    # Assign edge_label_index\n",
    "    train_data[edge_type].edge_label_index = train_edges\n",
    "    val_data[edge_type].edge_label_index = val_edges\n",
    "    test_data[edge_type].edge_label_index = test_edges\n",
    "\n",
    "    # Assign edge_label (positive class for all edges)\n",
    "    train_data[edge_type].edge_label = torch.ones(train_edges.shape[1])\n",
    "    val_data[edge_type].edge_label = torch.ones(val_edges.shape[1])\n",
    "    test_data[edge_type].edge_label = torch.ones(test_edges.shape[1])\n",
    "\n",
    "    # Negative sampling\n",
    "    num_nodes = max(subset_data[edge_type[0]].num_nodes, subset_data[edge_type[-1]].num_nodes)\n",
    "    neg_samples = negative_sampling(\n",
    "        edge_index=train_edges,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=min(train_edges.shape[1], 1000)\n",
    "    )\n",
    "    train_data[edge_type].neg_edge_index = neg_samples\n",
    "    train_data[edge_type].neg_edge_label = torch.zeros(neg_samples.shape[1])\n",
    "\n",
    "    print(f\"Edge type {edge_type}:\")\n",
    "    print(\"Number of nodes:\", subset_data[edge_type[0]].num_nodes)\n",
    "    print(\"Train edges:\", train_edges.shape[1])\n",
    "    print(\"Validation edges:\", val_edges.shape[1])\n",
    "    print(\"Test edges:\", test_edges.shape[1])\n",
    "    print(\"Edge labels in train:\", train_edges.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set 'set()'. Please explicitly set 'num_nodes' as an attribute of 'data[user]' to suppress this warning\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set 'set()'. Please explicitly set 'num_nodes' as an attribute of 'data[book]' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m num_nodes_dst \u001b[38;5;241m=\u001b[39m train_data[edge_type[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mnum_nodes\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Check for out-of-range indices\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m mask \u001b[38;5;241m=\u001b[39m (edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m num_nodes_src) \u001b[38;5;241m&\u001b[39m (edge_index[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m num_nodes_dst)\n\u001b[1;32m     14\u001b[0m train_data[edge_type]\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print edge index ranges for debugging\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Ensure num_nodes is correctly set for all node types\n",
    "for node_type in train_data.node_types:\n",
    "    if train_data[node_type].num_nodes is None:\n",
    "        train_data[node_type].num_nodes = train_data[node_type].x.shape[0]\n",
    "\n",
    "# Filter edge_index to ensure indices are valid\n",
    "for edge_type in train_data.edge_types:\n",
    "    edge_index = train_data[edge_type].edge_index\n",
    "    num_nodes_src = train_data[edge_type[0]].num_nodes\n",
    "    num_nodes_dst = train_data[edge_type[-1]].num_nodes\n",
    "\n",
    "    # Check for out-of-range indices\n",
    "    mask = (edge_index[0] < num_nodes_src) & (edge_index[1] < num_nodes_dst)\n",
    "    train_data[edge_type].edge_index = edge_index[:, mask]\n",
    "\n",
    "    # Print edge index ranges for debugging\n",
    "    print(f\"Edge type {edge_type}:\")\n",
    "    print(f\"Max index in edge_index[0]: {train_data[edge_type].edge_index[0].max().item()}\")\n",
    "    print(f\"Max index in edge_index[1]: {train_data[edge_type].edge_index[1].max().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Link Prediction, we have to add labels to indicate the edges to be predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_type in subset_data.edge_types:\n",
    "    # Use the same edge indices as edge_label_index\n",
    "    train_data[edge_type].edge_label_index = train_data[edge_type].edge_index\n",
    "    val_data[edge_type].edge_label_index = val_data[edge_type].edge_index\n",
    "    test_data[edge_type].edge_label_index = test_data[edge_type].edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "for edge_type in subset_data.edge_types:\n",
    "    num_nodes = subset_data[edge_type[0]].num_nodes  # Source node type: Users \n",
    "    neg_samples = negative_sampling(\n",
    "        edge_index=train_data[edge_type].edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=train_data[edge_type].edge_index.size(1)\n",
    "    )\n",
    "    train_data[edge_type].neg_edge_index = neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_type in train_data.node_types:\n",
    "    print(f\"Node type: {node_type}, num_nodes = {train_data[node_type].num_nodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type ('user', 'rates', 'book'):\n",
      "Number of nodes: None\n",
      "Train edges: 6777\n",
      "Validation edges: 2259\n",
      "Test edges: 2259\n",
      "Edge labels in train: 6777\n",
      "Edge type ('book', 'rated_by', 'user'):\n",
      "Number of nodes: None\n",
      "Train edges: 6777\n",
      "Validation edges: 2259\n",
      "Test edges: 2259\n",
      "Edge labels in train: 6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set 'set()'. Please explicitly set 'num_nodes' as an attribute of 'data[user]' to suppress this warning\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set 'set()'. Please explicitly set 'num_nodes' as an attribute of 'data[book]' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for edge_type in subset_data.edge_types:\n",
    "    print(f\"Edge type {edge_type}:\")\n",
    "    print(\"Number of nodes:\", train_data[edge_type[0]].num_nodes)\n",
    "    print(\"Train edges:\", train_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Validation edges:\", val_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Test edges:\", test_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Edge labels in train:\", train_data[edge_type].edge_label_index.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze6ILR13Pfpx"
   },
   "source": [
    "Now data object has two attributes of edge: `edge_index` and `edge_label_index`. `edge_index` denotes the graph structure used for performing message passing in GNN. `edge_label_index` denotes the edge index used to calculate loss in training set, or to evaluate the model in validation and test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIvYI-lPfkZ"
   },
   "source": [
    "Printing the statistics of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6bdhnhPPmBw"
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework originally uses a GCNConv but we can't because it is only for homogenous graphs. \n",
    "Using SAFEConv because need a layer that supports bipartite graphs. These layers are designed for graphs where edges connect nodes of different types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "import torch \n",
    "\n",
    "class HeteroGCN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = HeteroConv({\n",
    "            edge_type: SAGEonv((-1,-1), hidden_channels, add_self_loops=False) for edge_type in metadata[1]\n",
    "        }, aggr='sum')  # Aggregates messages across edge types\n",
    "        self.out_conv = HeteroConv({\n",
    "            edge_type: SAGEConv((-1,-1), out_channels, add_self_loops=False) for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # x_dict: Dict of node features for each node type\n",
    "        # edge_index_dict: Dict of edge_index for each edge type\n",
    "        x_dict = self.convs(x_dict, edge_index_dict)\n",
    "        x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = self.out_conv(x_dict, edge_index_dict)\n",
    "        return x_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['user', 'book'], [('user', 'rates', 'book'), ('book', 'rated_by', 'user')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wX5M2f1gQREs"
   },
   "outputs": [],
   "source": [
    "model = HeteroGCN(subset_data.metadata(), hidden_channels=128, out_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "BKR1W7rMQTpl"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycOaNK_1Qc8j"
   },
   "source": [
    "Similar as the what we do in the node classification task, we first apply the GCN model to produce the representation of each node in the graph. Usually we will use **inner product** to measure the similarity between two node representations to determine how likely it is for these two nodes to be connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkmpKFhoQfXu"
   },
   "source": [
    "#### Question 7 (5 points)\n",
    "\n",
    "Following the instruction and implement the function to calculate the inner product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qRUjKuvJQbLj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similairty: tensor([2.5647, 1.3936, 2.5647, 1.3936])\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity(node_embs, edge_index):\n",
    "    result = 0\n",
    "\n",
    "    # TODO: Define similarity function.\n",
    "    # 1. calculate the inner product between all the pairs in the edge_index\n",
    "    # Note: the shape of node_embs is [n, h] where n is the number of nodes, and h is the embedding size\n",
    "    # the shape of edge_index is [2, m] where m is the number of edges\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    result = (node_embs[edge_index[0]] *node_embs[edge_index[1]]).sum(dim=1)\n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "n, h = 5, 10  # number of nodes and embedding size\n",
    "node_embs = torch.rand(n, h)\n",
    "edge_index = torch.tensor([[0, 1, 2, 3],\n",
    "                           [2, 3, 0, 1]])  # compute the similarity of (0, 2), (1, 3), (2, 0), (3, 1)\n",
    "similarity = compute_similarity(node_embs, edge_index)\n",
    "print(\"Similairty:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llh52gpiQxLU"
   },
   "source": [
    "We optimize the model by minimizing the loss function. Here we consider the link prediction task as a binary classification task (edge exists or no), and apply binary cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "m_ue1HNBQw-d"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_olpc51XQ3hk"
   },
   "source": [
    "The edges in the graph will be taken as the positive examples with label=1 in the loss function. To prevent model from collapse, we usually will feed some **negative examples** to the loss function, which is the non-existing edges in the graph. The number of negative examples should equal to the number of positive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HkVK7WWQ7-Y"
   },
   "source": [
    "With the help of PyG, we can easily perform the negative sampling. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge type ('user', 'rates', 'book'):\n",
      "Number of nodes: None\n",
      "Train edges: 6777\n",
      "Validation edges: 2259\n",
      "Test edges: 2259\n",
      "Edge labels in train: 6777\n",
      "Edge type ('book', 'rated_by', 'user'):\n",
      "Number of nodes: None\n",
      "Train edges: 6777\n",
      "Validation edges: 2259\n",
      "Test edges: 2259\n",
      "Edge labels in train: 6777\n"
     ]
    }
   ],
   "source": [
    "for edge_type in subset_data.edge_types:\n",
    "    print(f\"Edge type {edge_type}:\")\n",
    "    print(\"Number of nodes:\", train_data[edge_type[0]].num_nodes)\n",
    "    print(\"Train edges:\", train_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Validation edges:\", val_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Test edges:\", test_data[edge_type].edge_index.shape[1])\n",
    "    print(\"Edge labels in train:\", train_data[edge_type].edge_label_index.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive edges\n",
      "tensor([[12919, 25169, 23894,  ..., 10945, 29776,   625],\n",
      "        [11678, 48532, 43909,  ..., 49199, 37157, 34127]])\n",
      "tensor([[11678, 48532, 43909,  ..., 49199, 37157, 34127],\n",
      "        [12919, 25169, 23894,  ..., 10945, 29776,   625]])\n",
      "============================\n",
      "Negative edge\n",
      "tensor([[ 575,  150, 3215,  ..., 2111, 2579, 1439],\n",
      "        [2732,  308, 1923,  ...,  325, 2454, 1441]])\n",
      "tensor([[2032, 3261, 2969,  ..., 2642,  859, 1161],\n",
      "        [2007, 2210, 3042,  ..., 3365,  474, 1140]])\n"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# for edge_type in data.edge_types:\n",
    "#     num_nodes = data[edge_type[0]].num_nodes  # Source node type: Users \n",
    "#     neg_samples = negative_sampling(\n",
    "#         edge_index=train_data[edge_type].edge_index,\n",
    "#         num_nodes=num_nodes,\n",
    "#         num_neg_samples=train_data[edge_type].edge_index.size(1)\n",
    "#     )\n",
    "#     train_data[edge_type].neg_edge_index = neg_samples\n",
    "\n",
    "\n",
    "t1 = subset_data.edge_types[0]\n",
    "t2 = subset_data.edge_types[1]\n",
    "print(\"Positive edges\")\n",
    "print(train_data[t1].edge_label_index)\n",
    "print(train_data[t2].edge_label_index)\n",
    "print(\"============================\")\n",
    "print(\"Negative edge\")\n",
    "print(train_data[t1].neg_edge_index)\n",
    "print((train_data[t2].neg_edge_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-wn9IvtPQoZF"
   },
   "outputs": [],
   "source": [
    "#code for homogenous graph \n",
    "# from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# neg_edge_index = negative_sampling(\n",
    "#       edge_index=train_data.edge_index,  # positive edges in the graph\n",
    "#       num_nodes=train_data.num_nodes,  # number of nodes\n",
    "#       num_neg_samples=5,  # number of negative examples\n",
    "#     )\n",
    "\n",
    "# print(\"shape of neg_edge_index:\", neg_edge_index.shape)  # [2, num_neg_samples]\n",
    "# print(\"negative examples:\", neg_edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVCP-wKERAwm"
   },
   "source": [
    "Positive examples (`edge_label_index`) will be assigned the label 1, and negative ones will be assigned the label 0. We can obtain the label of positive examples like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpuprVxhRQFg"
   },
   "source": [
    "Now we can construct training and testing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsfaRDYKRYf0"
   },
   "source": [
    "#### Question 8 (15 points)\n",
    "\n",
    "Please follow the instruction and implement a function that trains a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('user', 'rates', 'book'): Train=6777, Val=2259, Test=2259\n",
      "('book', 'rated_by', 'user'): Train=6777, Val=2259, Test=2259\n"
     ]
    }
   ],
   "source": [
    "for edge_type in train_data.edge_types:\n",
    "    print(f\"{edge_type}: Train={train_data[edge_type].edge_index.shape[1]}, \"\n",
    "          f\"Val={val_data[edge_type].edge_index.shape[1]}, Test={test_data[edge_type].edge_index.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "11oIZg5VRCqM"
   },
   "outputs": [],
   "source": [
    "# def train(model, data, optimizer, loss_fn):\n",
    "\n",
    "#     loss = 0\n",
    "\n",
    "#     # ARGS: \n",
    "#     #model - the heterogeneous GCN model: HeteroGCN \n",
    "#     #data - HeteroData object with x_dict, edge_index_dict and target labels zz\n",
    "#     #optimizer: Adam \n",
    "#     #criterion: Loss function, Binary Cross Entropy Loss\n",
    "\n",
    "#     ############# Your code here ############\n",
    "#     ## (~10 line of code)\n",
    "#     model.train() \n",
    "#     optimizer.zero_grad() \n",
    "\n",
    "#     #forward pass: pass node features and edge indices through the model \n",
    "#     out_dict = model(data.x_dict, data.edge_index_dict)\n",
    "#     print(\"Model output:\", {key: out.shape for key, out in out_dict.items()})\n",
    "\n",
    "#     #compute loss for all edge types (eg. link prediction)\n",
    "#     total_loss = 0 \n",
    "#     for edge_type in data.edge_types: \n",
    "#         if 'edge_label' in data[edge_type]: #check if edge labels exist for this type\n",
    "#             edge_label_index = data[edge_type].edge_label_index\n",
    "#             pred = compute_similarity(out_dict[edge_type[-1]], edge_label_index).view(-1)\n",
    "#             target = data[edge_type].edge_label#true labels for this edge type\n",
    "#             total_loss += loss_fn(pred, target)\n",
    "\n",
    "\n",
    "#     total_loss.backward() \n",
    "#     optimizer.step()\n",
    "\n",
    "#     #########################################\n",
    "\n",
    "#     return total_loss.item()\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Verify node features\n",
    "    for node_type in data.node_types:\n",
    "        if 'x' not in data[node_type]:\n",
    "            raise ValueError(f\"Node type {node_type} is missing 'x' (node features).\")\n",
    "\n",
    "    # Forward pass\n",
    "    out_dict = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    # Compute loss for all edge types\n",
    "    total_loss = 0\n",
    "    for edge_type in data.edge_types:\n",
    "        if 'edge_label' in data[edge_type]:\n",
    "            pred = out_dict[edge_type[-1]]  # Predictions for target node type\n",
    "            target = data[edge_type].edge_label\n",
    "            total_loss += criterion(pred, target)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZrd0esJRiEI"
   },
   "source": [
    "We usually use [AUC score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) to evaluate the performance of model on binary classification task. The test function is as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "PC7hxxZNRe-0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    total_auc = 0 \n",
    "    total_edges = 0 \n",
    "\n",
    "    for edge_type in data.edge_types: \n",
    "        if 'edge_label' in data[edge_type]:#ensure labels exist \n",
    "            #perform message passing and get predictions for this edge type \n",
    "            out = model(data.x_dict, data.edge_index_dict)\n",
    "            node_embeddings = out[edge_type[-1]]# target node type embeddings\n",
    "\n",
    "            #compute similarity for edges in edge_label_index\n",
    "            edge_label_index = data[edge_type].edge_label_index\n",
    "            predictions = compute_similarity(out[edge_type[-1]], edge_label_index).view(-1).sigmoid()\n",
    "            print(\"Predictions shape:\", predictions.shape)\n",
    "            print(\"Edge labels shape:\", edge_label_index.shape)\n",
    "\n",
    "\n",
    "            #compute AUC for this edge type \n",
    "            edge_label = data[edge_type].edge_label\n",
    "            auc = roc_auc_score(edge_label.cpu().numpy(), predictions.cpu().numpy())\n",
    "            \n",
    "            # Weighted aggregation of AUC scores\n",
    "            num_edges = edge_label.size(0)\n",
    "            total_auc += auc * num_edges\n",
    "            total_edges += num_edges\n",
    "\n",
    "    # Compute the final AUC (weighted average across edge types)\n",
    "    return total_auc / total_edges if total_edges > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLkW7ZlBRnXB"
   },
   "source": [
    "Now we can start to train our model based on `train` and `test` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "rCVflGNBRnCR"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Tried to collect 'x' but did not find any occurrences of it in any node and/or edge type\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_auc \u001b[38;5;241m=\u001b[39m final_test_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(model, train_data, optimizer, loss_fn)\n\u001b[1;32m      7\u001b[0m     valid_auc \u001b[38;5;241m=\u001b[39m test(model, val_data)\n\u001b[1;32m      8\u001b[0m     test_auc \u001b[38;5;241m=\u001b[39m test(model, test_data)\n",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#forward pass: pass node features and edge indices through the model \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m out_dict \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx_dict, data\u001b[38;5;241m.\u001b[39medge_index_dict)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#compute loss for all edge types (eg. link prediction)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/hetero_data.py:161\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_store, key)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dict$\u001b[39m\u001b[38;5;124m'\u001b[39m, key)):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch_geometric/data/hetero_data.py:565\u001b[0m, in \u001b[0;36mHeteroData.collect\u001b[0;34m(self, key, allow_empty)\u001b[0m\n\u001b[1;32m    563\u001b[0m         mapping[subtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(store, key)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_empty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapping) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to collect \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but did not find any \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrences of it in any node and/or edge type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Tried to collect 'x' but did not find any occurrences of it in any node and/or edge type\""
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data, optimizer, loss_fn)\n",
    "    \n",
    "    valid_auc = test(model, val_data)\n",
    "    test_auc = test(model, test_data)\n",
    "    if valid_auc > best_val_auc:\n",
    "        best_val_auc = valid_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {valid_auc:.4f}, Test: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
