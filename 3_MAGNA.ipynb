{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xjtuwgt/GNN-MAGNA\n",
    "\n",
    "```GATConv(in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0.0, add_self_loops=True, edge_dim=None, fill_value='mean', bias=True, residual=False)``` \n",
    "\n",
    "- The graph attentional operator from the Graph Attention Networks paper \n",
    "- in_channels - size of each input sample to dervice the size from teh first input to the forward method \n",
    "- out_channel - size of each output samle \n",
    "- heads - number of multi head attentions \n",
    "- concat - if set to false, the multi head attentions are averaged instead of concatenated \n",
    "- negative slope - leaky ReLU angle of the negative slope \n",
    "- dropout - dropout probability of the normalized attention coefficients which exposes each node to stochastically sampled neighborhood during training \n",
    "- add_selfloops - add self loops to input graph \n",
    "- edge_dim - edge feature dimensionality in case there are any \n",
    "- fill value - \n",
    "\n",
    "\n",
    "\n",
    "```nn.ModuleList```\n",
    "```nn.LayerNorm```\n",
    "```nn.Linear```\n",
    "```nn.Embedding```\n",
    "```nn.Parameter```\n",
    "```torch.bmm```\n",
    "\n",
    "\n",
    "# 2.2 Multi Hop Attention Diffusion\n",
    "\n",
    "Attention diffusion to compute the multi-hop attention directly, which operates on the MAGNA's attention scores at each layer. \n",
    "\n",
    "Input ($v_i, r_k, v_j$) \n",
    "- $v_i, v_j$ are nodes \n",
    "- $r_k$ is edge type \n",
    "\n",
    "1. MAGNA computes the attention scores on all edges \n",
    "2. Attention diffusion module then computes the attention values between pairs of nodes not directly connected by an edge via diffusion process \n",
    "\n",
    "## Step 1. Edge Attention Computation \n",
    "\n",
    "At each layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNA(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads=8, alpha=0.1, num_layers=3):\n",
    "        super(MAGNA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.alpha = alpha\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(GATConv(in_channels, out_channels, heads=num_heads, concat=True, dropout=0.6))\n",
    "            in_channels = out_channels * num_heads\n",
    "        \n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(out_channels * num_heads) for _ in range(num_layers)])\n",
    "        self.feed_forwards = nn.ModuleList([nn.Linear(out_channels * num_heads, out_channels * num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "#========IMPORTANT====================================================\n",
    "    def attention_diffusion(self, A, H):\n",
    "        # Approximating attention diffusion with recursive updates\n",
    "        Z = H.clone()\n",
    "        for _ in range(6):  # Assuming a fixed hop count K = 6\n",
    "            Z = (1 - self.alpha) * torch.matmul(A, Z) + self.alpha * H\n",
    "        return Z\n",
    "#========IMPORTANT====================================================\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            A = F.softmax(torch.matmul(x, x.T), dim=-1)  # Compute the attention matrix\n",
    "            x = self.attention_diffusion(A, x)\n",
    "            x = self.norms[i](x + x)  # Residual connection\n",
    "            x = F.relu(self.feed_forwards[i](x)) + x  # Feed forward with residual connection\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
