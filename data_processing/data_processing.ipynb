{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Exploratory Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be some exploratory work for processing the data. This will look at both the data for the Amazon data set and the Tao Bao data set. \n",
    "\n",
    "we will start off with looking at the data for the Amazon Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the necessary libraries for the Amazon data set\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import AmazonBook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data exploration for the taobao datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taobao_userbehavior_df = pd.read_csv(\"taobao/raw/UserBehavior.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0        1        2   3           4\n",
      "0  1  2268318  2520377  pv  1511544070\n",
      "1  1  2333346  2520771  pv  1511561733\n",
      "2  1  2576651   149192  pv  1511572885\n",
      "3  1  3830808  4181361  pv  1511593493\n",
      "4  1  4365585  2520377  pv  1511596146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>behavior_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2268318</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511544070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2333346</td>\n",
       "      <td>2520771</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511561733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2576651</td>\n",
       "      <td>149192</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511572885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3830808</td>\n",
       "      <td>4181361</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511593493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4365585</td>\n",
       "      <td>2520377</td>\n",
       "      <td>pv</td>\n",
       "      <td>1511596146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  category_id behavior_type   timestamp\n",
       "0        1  2268318      2520377            pv  1511544070\n",
       "1        1  2333346      2520771            pv  1511561733\n",
       "2        1  2576651       149192            pv  1511572885\n",
       "3        1  3830808      4181361            pv  1511593493\n",
       "4        1  4365585      2520377            pv  1511596146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(taobao_userbehavior_df.head())\n",
    "taobao_userbehavior_df.columns = ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n",
    "taobao_userbehavior_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the data.pt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_379287/3168682384.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  taobao_pt = torch.load('./taobao/processed/data.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "3\n",
      "<class 'dict'>\n",
      "<class 'NoneType'>\n",
      "<class 'abc.ABCMeta'>\n"
     ]
    }
   ],
   "source": [
    "taobao_pt = torch.load('./taobao/processed/data.pt')\n",
    "print(type(taobao_pt))\n",
    "print(len(taobao_pt))\n",
    "print(type(taobao_pt[0]))\n",
    "print(type(taobao_pt[1]))\n",
    "print(type(taobao_pt[2]))\n",
    "taobao_data = taobao_pt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_global_store': {},\n",
       " 'user': {'num_nodes': 987991},\n",
       " 'item': {'num_nodes': 4161138},\n",
       " 'category': {'num_nodes': 9437},\n",
       " ('user',\n",
       "  'to',\n",
       "  'item'): {'edge_index': tensor([[      0,       0,       0,  ...,  970447,  970447,  970447],\n",
       "          [1827766, 1880345, 2076699,  ..., 2939548, 1534057, 2978718]]), 'time': tensor([1511544070, 1511561733, 1511572885,  ..., 1512293792, 1512293827,\n",
       "          1512293891]), 'behavior': tensor([0, 0, 0,  ..., 0, 0, 0])},\n",
       " ('item',\n",
       "  'to',\n",
       "  'category'): {'edge_index': tensor([[1827766, 1880345, 2076699,  ...,  848356,  522299, 2015151],\n",
       "          [   4564,    4565,     259,  ...,    4637,    4565,    8438]])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taobao_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't think that we can use the already processed data.pt --> still need to preprocess the data for the appropriate knowledge graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking specifically into the amazon-book datasets to better understand what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_entity_df = pd.read_csv('./amazon-book/entity_list.txt', delimiter=' ', on_bad_lines='skip')\n",
    "amazon_item_list_df = pd.read_csv('./amazon-book/item_list.txt', delimiter  = ' ', on_bad_lines = 'skip')\n",
    "amazon_relation_df = pd.read_csv('./amazon-book/relation_list.txt', delimiter = ' ', on_bad_lines = 'skip')\n",
    "amazon_user_df = pd.read_csv('./amazon-book/user_list.txt', delimiter = ' ', on_bad_lines = 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83460, 2)\n",
      "(24915, 3)\n",
      "(39, 2)\n",
      "(70679, 2)\n"
     ]
    }
   ],
   "source": [
    "print(amazon_entity_df.shape)\n",
    "print(amazon_item_list_df.shape)\n",
    "print(amazon_relation_df.shape)\n",
    "print(amazon_user_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon entity mapping\n",
      "83460\n",
      "83460\n",
      "      org_id  remap_id\n",
      "0  m.045wq1q         0\n",
      "1   m.03_28m         1\n",
      "2  m.0h2q1cq         2\n",
      "3  m.04y9jxd         3\n",
      "4   m.060c1r         4\n",
      "amazon item mapping\n",
      "24915\n",
      "24915\n",
      "24915\n",
      "       org_id  remap_id freebase_id\n",
      "0  0553092626         0   m.045wq1q\n",
      "1  0393316041         1    m.03_28m\n",
      "2  038548254X         2   m.0h2q1cq\n",
      "3  0385307756         3   m.04y9jxd\n",
      "4  038531258X         4    m.060c1r\n",
      "amazon relation mapping\n",
      "39\n",
      "39\n",
      "                                              org_id  remap_id\n",
      "0        http://rdf.freebase.com/ns/type.object.type         0\n",
      "1      http://rdf.freebase.com/ns/type.type.instance         1\n",
      "2  http://rdf.freebase.com/ns/book.written_work.c...         2\n",
      "3    http://www.w3.org/1999/02/22-rdf-syntax-ns#type         3\n",
      "4  http://rdf.freebase.com/ns/kg.object_profile.p...         4\n",
      "amazon user mapping\n",
      "70679\n",
      "70679\n",
      "           org_id  remap_id\n",
      "0  A3RTKL9KB8KLID         0\n",
      "1  A38LAIK2N83NH0         1\n",
      "2  A3PPXVR5J6U2JD         2\n",
      "3  A2ULDDL3MLJPUR         3\n",
      "4  A2I6MHMAZZDCRX         4\n"
     ]
    }
   ],
   "source": [
    "print('amazon entity mapping')\n",
    "print(len(amazon_entity_df['org_id'].unique()))\n",
    "print(len(amazon_entity_df['remap_id'].unique()))\n",
    "print(amazon_entity_df.head())\n",
    "\n",
    "print('amazon item mapping')\n",
    "print(len(amazon_item_list_df['org_id'].unique()))\n",
    "print(len(amazon_item_list_df['remap_id'].unique()))\n",
    "print(len(amazon_item_list_df['freebase_id'].unique()))\n",
    "print(amazon_item_list_df.head())\n",
    "\n",
    "print('amazon relation mapping')\n",
    "print(len(amazon_relation_df['org_id'].unique()))\n",
    "print(len(amazon_relation_df['remap_id'].unique()))\n",
    "print(amazon_relation_df.head())\n",
    "\n",
    "print('amazon user mapping')\n",
    "print(len(amazon_user_df['org_id'].unique()))\n",
    "print(len(amazon_user_df['remap_id'].unique()))\n",
    "print(amazon_user_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This essentially tells me that there are only unique mappings here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the kg dataset: (2557746, 3)\n",
      "    head  relation   tail\n",
      "0  24915         0  24916\n",
      "1  24917         1   5117\n",
      "2  24918         0  24917\n",
      "3  24919         1  24920\n",
      "4  24921         2  24922\n",
      "unique relations: 39\n",
      "unique heads: 113308\n",
      "unique tails: 113479\n",
      "max heads: 113486\n",
      "max tails: 113486\n"
     ]
    }
   ],
   "source": [
    "amazon_kg = pd.read_csv('./amazon-book/kg_final.txt', delimiter = ' ', on_bad_lines = 'skip', header = None)\n",
    "\n",
    "print('shape of the kg dataset:', amazon_kg.shape)\n",
    "amazon_kg.columns =['head', 'relation', 'tail']\n",
    "print(amazon_kg.head())\n",
    "print('unique relations:', len(amazon_kg['relation'].unique()))\n",
    "print('unique heads:', len(amazon_kg['head'].unique()))\n",
    "print('unique tails:', len(amazon_kg['tail'].unique()))\n",
    "print('max heads:', (amazon_kg['head'].max()))\n",
    "print('max tails:', (amazon_kg['tail'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in here, there are more unique heads and tails than there are in terms of the actual mappings present in each of the entity/item/user list txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_train = pd.read_csv('./amazon-book/train.txt', delimiter=' ', on_bad_lines='skip', header = None)\n",
    "# amazon_test = pd.read_csv('./amazon-book/test.txt', delimiter  = ' ', on_bad_lines = 'skip', header = None)\n",
    "\n",
    "def _load_ratings(file_name):\n",
    "    user_dict = dict()\n",
    "    inter_mat = list()\n",
    "\n",
    "    lines = open(file_name, 'r').readlines()\n",
    "    for l in lines:\n",
    "        tmps = l.strip()\n",
    "        inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "        u_id, pos_ids = inters[0], inters[1:]\n",
    "        pos_ids = list(set(pos_ids))\n",
    "\n",
    "        for i_id in pos_ids:\n",
    "            inter_mat.append([u_id, i_id])\n",
    "\n",
    "        if len(pos_ids) > 0:\n",
    "            user_dict[u_id] = pos_ids\n",
    "    return np.array(inter_mat), user_dict\n",
    "    \n",
    "\n",
    "amazon_train_array, amazon_train_dict = _load_ratings('./amazon-book/train.txt')\n",
    "amazon_test_array, amazon_test_dict = _load_ratings('./amazon-book/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amazon_train_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_379287/3476230785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_train_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_test_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_train_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_test_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'amazon_train_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(amazon_train_dict))\n",
    "print(len(amazon_test_dict))\n",
    "\n",
    "print(len(amazon_train_array))\n",
    "print(len(amazon_test_array))\n",
    "\n",
    "print(amazon_train_array.shape)\n",
    "print('max item id train:', amazon_train_array[:,1].max())\n",
    "print('max item id test:', amazon_test_array[:,1].max())\n",
    "\n",
    "print('max user id train:', amazon_train_array[:,0].max())\n",
    "print('max user id test:', amazon_test_array[:,0].max())\n",
    "\n",
    "## basically every user shows up here\n",
    "print(len(amazon_test_array)/len(amazon_train_array))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
